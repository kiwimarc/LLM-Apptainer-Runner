Bootstrap: docker
From:  nvidia/cuda:12.4.1-runtime-ubuntu22.04

%labels
    Author Marc Cummings
    Version 1.0
    Model LLaMA-3.3-70B-Instruct
    Type Text-only
    Build for H100 (sm_90) - FlashAttn v2.8.3

%environment
    export MODEL_PATH=/opt/models/llama3-70b
    export PYTHONUNBUFFERED=1
    export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

%post
    # Basic utilities
    apt-get update && apt-get install -y \
        python3 python3-pip git curl wget vim \
        && rm -rf /var/lib/apt/lists/*

    # Install Python dependencies
    pip3 install --upgrade pip

    # Install PyTorch
    pip3 install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu124

    # Install Flash Attention via PRE-BUILT WHEEL
    # Target: CUDA 12 | Torch 2.6 | Python 3.10 
    pip3 install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

    # Install other dependencies
    pip3 install transformers accelerate sentencepiece

    # Create model directory
    mkdir -p /opt/models/

%files
    # Copy downloaded LLaMA-3.3 70B model into the container
    ./models/Llama-3.3-70B-Instruct /opt/models/llama3-70b
    ./containers/llama3-70b/run.py /opt/run.py

%runscript
    exec python3 /opt/run.py "$@"